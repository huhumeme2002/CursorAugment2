# BÃO CÃO ÄIá»€U TRA: Váº¤N Äá»€ "NHá»š NGá»® Cáº¢NH KÃ‰M"

**NgÃ y Ä‘iá»u tra:** 2026-02-11
**Team:** context-memory-investigation
**Tráº¡ng thÃ¡i:** HoÃ n thÃ nh

---

## ğŸ¯ TÃ“M Táº®T ÄIá»€U TRA

**Káº¿t luáº­n chÃ­nh:** Proxy lÃ  **STATELESS** - khÃ´ng lÆ°u trá»¯ lá»‹ch sá»­ há»™i thoáº¡i. Váº¥n Ä‘á» "nhá»› ngá»¯ cáº£nh kÃ©m" **KHÃ”NG PHáº¢I** do lá»—i trong proxy code, mÃ  do:

1. **Client application khÃ´ng gá»­i Ä‘á»§ conversation history** (nguyÃªn nhÃ¢n kháº£ dÄ© nháº¥t)
2. **Backend API xá»­ lÃ½ context khÃ´ng tá»‘t**
3. **System prompt quÃ¡ dÃ i chiáº¿m tokens**

---

## âœ… ÄÃƒ XÃC NHáº¬N KHÃ”NG PHáº¢I NGUYÃŠN NHÃ‚N

### 1. Messages Bá»‹ Cáº¯t/Filter (Researcher-2)
**Káº¿t quáº£:** âœ… KHÃ”NG CÃ“ Váº¤N Äá»€

- Proxy giá»¯ nguyÃªn 100% messages array tá»« client â†’ backend
- KhÃ´ng cÃ³ truncation, filtering, hoáº·c middleware can thiá»‡p
- Chá»‰ cÃ³ system prompt injection (khÃ´ng áº£nh hÆ°á»Ÿng user/assistant messages)
- Smart usage counting phÃ¢n biá»‡t user messages vs tool results
- Bypass system prompt cÃ³ sáºµn qua `supperapi.store` URLs hoáº·c profile flag

**File kiá»ƒm tra:** `api/proxy.ts`

---

### 2. Proxy LÆ°u Trá»¯ Conversation Sai (Researcher-1)
**Káº¿t quáº£:** âœ… KHÃ”NG CÃ“ Váº¤N Äá»€

**PhÃ¡t hiá»‡n quan trá»ng:** Proxy hoÃ n toÃ n stateless

- Redis chá»‰ lÆ°u: API keys, settings, profiles, metrics, announcements
- **KHÃ”NG** lÆ°u conversation history, messages, hoáº·c chat state
- Client pháº£i gá»­i full context trong má»—i request
- ÄÃ¢y lÃ  kiáº¿n trÃºc chuáº©n cá»§a OpenAI-compatible proxy

**Redis Schema hiá»‡n táº¡i:**
```
api_key:{key_id} â†’ RedisKeyData (expiry, daily_limit, usage)
api_profile:{id} â†’ APIProfile (backend config)
backup_profile:{id} â†’ BackupProfile (fallback config)
model_config:{name} â†’ ModelConfig (system prompts)
settings â†’ Global configuration
concurrency:{profile_id} â†’ Number (concurrent requests)
announcement:{id} â†’ Announcement (system notifications)
metrics:* â†’ Performance metrics
```

**KhÃ´ng cÃ³:** `conversation:*`, `chat_history:*`, `messages:*`, `thread:*`

**File kiá»ƒm tra:** `lib/redis.ts`, `lib/types.ts`

---

### 3. Session Tracking Issues (Researcher-3)
**Káº¿t quáº£:** âœ… KHÃ”NG CÃ“ Váº¤N Äá»€ (nhÆ°ng cÃ³ bugs khÃ¡c)

**Conversation tracking:**
- Correlation ID chá»‰ dÃ¹ng cho logging/debugging (per-request)
- Session interface Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ°ng **KHÃ”NG Sá»¬ Dá»¤NG**
- KhÃ´ng cÃ³ conversation_id, thread_id, hoáº·c chat_id tracking
- Má»—i request hoÃ n toÃ n Ä‘á»™c láº­p
- **KHÃ”NG cÃ³ bug nÃ o gÃ¢y nháº§m láº«n conversation data giá»¯a users**

**âš ï¸ BUG PHÃT HIá»†N (khÃ´ng liÃªn quan Ä‘áº¿n context memory):**

**Bug #1: Correlation ID Storage KhÃ´ng An ToÃ n** (lib/logger.ts:50-65)
```typescript
const correlationIdStorage = new Map<string, string>();

export function setCorrelationId(id: string): void {
    correlationIdStorage.set('current', id);  // âŒ GLOBAL KEY!
}
```

**Váº¥n Ä‘á»:**
- Sá»­ dá»¥ng key cá»‘ Ä‘á»‹nh `'current'` cho Táº¤T Cáº¢ requests
- Trong mÃ´i trÆ°á»ng concurrent, requests cÃ³ thá»ƒ ghi Ä‘Ã¨ láº«n nhau
- Logs cÃ³ thá»ƒ bá»‹ gáº¯n sai correlation ID
- **KhÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n conversation data**, chá»‰ áº£nh hÆ°á»Ÿng debugging

**Khuyáº¿n nghá»‹:** Thay báº±ng AsyncLocalStorage (nhÆ° comment trong code Ä‘Ã£ gá»£i Ã½)

**Bug #2: Session Interface KhÃ´ng ÄÆ°á»£c Sá»­ Dá»¥ng**
- Interface Ä‘á»‹nh nghÄ©a trong `lib/types.ts:2-11` nhÆ°ng khÃ´ng cÃ³ code nÃ o dÃ¹ng
- Functions `activateDevice()` vÃ  `isDeviceActivated()` Ä‘Ã£ deprecated
- Khuyáº¿n nghá»‹: XÃ³a code khÃ´ng dÃ¹ng hoáº·c implement Ä‘Ãºng cÃ¡ch

**File kiá»ƒm tra:** `lib/logger.ts`, `lib/types.ts`, `api/proxy.ts`

---

### 4. Context Limits Trong Proxy (Researcher-5)
**Káº¿t quáº£:** âœ… KHÃ”NG CÃ“ Váº¤N Äá»€

- Proxy **KHÃ”NG** cÃ³ token counting hay truncation logic
- KhÃ´ng cÃ³ context window validation
- Táº¥t cáº£ parameters (max_tokens, messages, temperature) Ä‘Æ°á»£c forward nguyÃªn váº¹n
- Chá»‰ cÃ³ 1 giá»›i háº¡n: System prompt tá»‘i Ä‘a 10K characters

**Context limits do backend API quyáº¿t Ä‘á»‹nh:**
- Claude API: ~200K tokens (Opus)
- GPT API: ~128K tokens (GPT-4)
- Gemini API: ~1M+ tokens
- Proxy dá»±a vÃ o backend Ä‘á»ƒ reject oversized requests

**Potential issues:**
1. System prompt overhead (lÃªn Ä‘áº¿n 10K chars)
2. Sai `model_actual` mapping (trá» Ä‘áº¿n model cÃ³ context nhá» hÆ¡n)
3. Backend API restrictions
4. Backend service configuration limits

**File kiá»ƒm tra:** `lib/types.ts`, `api/proxy.ts`

---

### 5. Caching Strategy Impact (Researcher-4)
**Káº¿t quáº£:** âœ… KHÃ”NG CÃ“ Váº¤N Äá»€

**LRU Cache chá»‰ lÆ°u configuration data:**
- API Profiles (60s TTL, max 100 entries)
- Backup Profiles (60s TTL)
- Model Configs (120s TTL)
- Settings (30s TTL, separate cache)

**KhÃ´ng cache:**
- API Keys (luÃ´n fetch fresh tá»« Redis)
- Session data
- Conversation history (khÃ´ng tá»“n táº¡i)
- User-specific data

**Potential stale data issues:**
- Profile changes máº¥t 60s Ä‘á»ƒ propagate (minor UX issue)
- Settings changes máº¥t 30s Ä‘á»ƒ propagate
- **KhÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n conversation flow**

**Frontend Analysis:**
- `public/admin/app.js`: Admin dashboard, khÃ´ng cÃ³ chat UI
- `public/user/index.html`: Key status checker, khÃ´ng cÃ³ chat UI
- **KhÃ´ng cÃ³ chat client trong codebase nÃ y**
- Proxy Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ dÃ¹ng vá»›i external clients (Cursor, Continue.dev, etc.)

**File kiá»ƒm tra:** `lib/redis.ts`, `public/admin/app.js`, `public/user/index.html`

---

## ğŸ¯ NGUYÃŠN NHÃ‚N KHáº¢ DÄ¨

VÃ¬ proxy lÃ  stateless vÃ  khÃ´ng can thiá»‡p vÃ o messages, váº¥n Ä‘á» "nhá»› ngá»¯ cáº£nh kÃ©m" chá»‰ cÃ³ thá»ƒ do:

### 1. âš ï¸ Client Application KhÃ´ng Gá»­i Äá»§ Conversation History (Kháº£ nÄƒng cao nháº¥t)

**LÃ½ do:**
- Proxy khÃ´ng lÆ°u conversation history
- Client pháº£i gá»­i full messages array trong má»—i request
- External chat applications (Cursor, TypingMind, Chatbox AI) cÃ³ thá»ƒ:
  - Giá»›i háº¡n sá»‘ messages gá»­i Ä‘i
  - Truncate conversation history Ä‘á»ƒ tiáº¿t kiá»‡m tokens
  - CÃ³ bugs trong conversation management
  - KhÃ´ng implement context window management Ä‘Ãºng cÃ¡ch

**CÃ¡ch kiá»ƒm tra:**
- Log incoming `requestBody.messages` trong `api/proxy.ts`
- Kiá»ƒm tra xem client cÃ³ gá»­i Ä‘á»§ messages khÃ´ng
- So sÃ¡nh vá»›i conversation history thá»±c táº¿ trong client UI

**CÃ¡ch kháº¯c phá»¥c:**
- Náº¿u lÃ  client cá»§a báº¡n: Fix conversation management logic
- Náº¿u lÃ  third-party client: KhÃ´ng thá»ƒ fix, chá»‰ cÃ³ thá»ƒ document limitation

---

### 2. Backend API Issues

**Kháº£ nÄƒng:**
- Backend khÃ´ng xá»­ lÃ½ context tá»‘t
- Model Ä‘Æ°á»£c chá»n cÃ³ context window nhá»
- Backend service cÃ³ configuration limits
- Backend API rate limiting hoáº·c throttling

**CÃ¡ch kiá»ƒm tra:**
- Log backend responses trong `api/proxy.ts`
- Kiá»ƒm tra error messages tá»« backend
- Test trá»±c tiáº¿p vá»›i backend API (bypass proxy)

**CÃ¡ch kháº¯c phá»¥c:**
- Chá»n model cÃ³ context window lá»›n hÆ¡n
- Äiá»u chá»‰nh backend configuration
- Switch sang backend profile khÃ¡c

---

### 3. System Prompt QuÃ¡ DÃ i

**Kháº£ nÄƒng:**
- System prompt chiáº¿m quÃ¡ nhiá»u tokens (max 10K chars)
- Giáº£m khÃ´ng gian cho conversation history
- Backend API reject request vÃ¬ quÃ¡ dÃ i

**CÃ¡ch kiá»ƒm tra:**
- Kiá»ƒm tra system prompt length trong admin panel
- Test vá»›i system prompt ngáº¯n hÆ¡n
- Monitor backend error responses

**CÃ¡ch kháº¯c phá»¥c:**
- RÃºt gá»n system prompt
- Sá»­ dá»¥ng `disable_system_prompt_injection` flag
- Bypass system prompt cho specific profiles

---

## ğŸ”§ KHUYáº¾N NGHá»Š GIáº¢I PHÃP

### Giáº£i phÃ¡p ngáº¯n háº¡n (Investigation/Debugging):

1. **ThÃªm logging chi tiáº¿t trong api/proxy.ts:**
   ```typescript
   // Log incoming messages count
   console.log(`[PROXY] Received ${requestBody.messages.length} messages from client`);

   // Log first and last message for context
   console.log(`[PROXY] First message:`, requestBody.messages[0]);
   console.log(`[PROXY] Last message:`, requestBody.messages[requestBody.messages.length - 1]);

   // Log system prompt length
   console.log(`[PROXY] System prompt length: ${systemPrompt?.length || 0} chars`);
   ```

2. **ThÃªm debug endpoint Ä‘á»ƒ inspect requests:**
   ```typescript
   // api/debug/last-request.ts
   // Store last N requests in memory for debugging
   ```

3. **Monitor backend responses:**
   - Log error messages tá»« backend
   - Track context-related errors (token limit exceeded, etc.)

4. **Test vá»›i different clients:**
   - Test vá»›i curl (gá»­i manual messages array)
   - Test vá»›i different chat applications
   - So sÃ¡nh káº¿t quáº£

---

### Giáº£i phÃ¡p dÃ i háº¡n (Optional Features):

**âš ï¸ LÆ¯U Ã: Chá»‰ implement náº¿u thá»±c sá»± cáº§n thiáº¿t**

#### Option 1: Server-Side Conversation Storage (Breaking Change)

**Pros:**
- Client khÃ´ng cáº§n quáº£n lÃ½ conversation history
- Proxy cÃ³ thá»ƒ optimize context window
- CÃ³ thá»ƒ implement conversation summarization

**Cons:**
- PhÃ¡ vá»¡ kiáº¿n trÃºc stateless hiá»‡n táº¡i
- TÄƒng Redis storage costs
- TÄƒng complexity
- KhÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i OpenAI-compatible clients

**Implementation:**
```typescript
// lib/types.ts
export interface Conversation {
    conversation_id: string;
    api_key: string;
    messages: Array<{role: string; content: string}>;
    created_at: number;
    last_activity: number;
}

// Redis schema
conversation:{conversation_id} â†’ Conversation
```

**KHÃ”NG KHUYáº¾N NGHá»Š** - PhÃ¡ vá»¡ compatibility vá»›i existing clients

---

#### Option 2: Context Window Management (Recommended)

**Pros:**
- Giá»¯ nguyÃªn stateless architecture
- Tá»± Ä‘á»™ng truncate old messages khi vÆ°á»£t quÃ¡ limit
- Transparent cho client

**Cons:**
- Máº¥t context cÅ©
- Cáº§n implement smart truncation logic

**Implementation:**
```typescript
// api/proxy.ts
function truncateMessages(messages: any[], maxTokens: number): any[] {
    // Keep system message + recent messages
    // Estimate tokens (rough: 1 token â‰ˆ 4 chars)
    // Truncate from middle, keep first and last messages
}
```

**KHUYáº¾N NGHá»Š** - Náº¿u muá»‘n thÃªm feature nÃ y

---

#### Option 3: Conversation Summarization

**Pros:**
- Giá»¯ context quan trá»ng
- Giáº£m token usage
- Improve long conversations

**Cons:**
- Cáº§n call LLM Ä‘á»ƒ summarize (cost + latency)
- CÃ³ thá»ƒ máº¥t thÃ´ng tin quan trá»ng
- Complex implementation

**KHÃ”NG KHUYáº¾N NGHá»Š** - QuÃ¡ phá»©c táº¡p cho use case nÃ y

---

## ğŸ“Š Káº¾T LUáº¬N

### Findings chÃ­nh:

1. âœ… **Proxy code KHÃ”NG cÃ³ lá»—i** vá» xá»­ lÃ½ conversation context
2. âœ… **Messages array Ä‘Æ°á»£c forward nguyÃªn váº¹n** tá»« client â†’ backend
3. âœ… **KhÃ´ng cÃ³ bugs gÃ¢y nháº§m láº«n data giá»¯a users**
4. âš ï¸ **CÃ³ bug trong correlation ID tracking** (khÃ´ng áº£nh hÆ°á»Ÿng functionality)
5. ğŸ¯ **Váº¥n Ä‘á» "nhá»› ngá»¯ cáº£nh kÃ©m" ráº¥t cÃ³ thá»ƒ do client application**

### Next steps:

1. **Immediate:** ThÃªm logging Ä‘á»ƒ xÃ¡c Ä‘á»‹nh client cÃ³ gá»­i Ä‘á»§ messages khÃ´ng
2. **Short-term:** Fix correlation ID bug (AsyncLocalStorage)
3. **Medium-term:** XÃ³a unused Session interface code
4. **Long-term:** Consider context window management feature (optional)

### CÃ¢u há»i cáº§n tráº£ lá»i:

1. User Ä‘ang dÃ¹ng client application nÃ o? (Cursor, TypingMind, Chatbox AI, custom?)
2. CÃ³ thá»ƒ access logs cá»§a client application khÃ´ng?
3. User cÃ³ thá»ƒ test vá»›i curl Ä‘á»ƒ verify proxy behavior khÃ´ng?
4. Backend API nÃ o Ä‘ang Ä‘Æ°á»£c dÃ¹ng? (Claude, GPT, Gemini?)

---

**BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o bá»Ÿi:** Team context-memory-investigation
**Researchers:** researcher-1, researcher-2, researcher-3, researcher-4, researcher-5
